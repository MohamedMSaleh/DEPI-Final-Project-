 
 pg. 1 AI & Data Science Track – Round 2  
Project Instructions for Students: - 
The graduation project is a key requirement for obtaining the Digital Egypt Pioneers Initiative 
Completion Certificate. 
• Students are free to choose any of the ideas listed in the project booklet for their respective 
career track without any restrictions "With the management of the initiative being duly 
informed.", they are able to choose other ideas not listed in the booklet, but it should go in the 
same format of the ideas given. 
• The project is a group assignment, and teams should consist of 4 to 6 students. 
• Within a maximum of one week from the announcement of the project booklet, students must 
form their groups and inform the instructor. If they fail to do so, the instructor has the right to 
assign groups randomly and announce the team members. 
• Students must divide the work responsibilities within the group and inform the instructor within 
two weeks of the project booklet announcement. During the final presentation, each group must 
demonstrate the work completed and each member's responsibility for their assigned tasks. 
• The final evaluation will be based on the final presentation, which must include the students' 
adherence to the deliverables and the distribution of tasks among team members. 
pg. 4 
AI & Data Science Track – Round 2  
Project: Real-time IoT Data Pipeline 
(Tied to Module 5: Data Pipelines + Module 6: Big Data Processing) 
Project Overview: 
Students will build a pipeline that simulates sensor data (temperature, humidity) and processes it using 
batch and streaming techniques. This introduces orchestration, real-time analytics, and cloud-native 
processing. 
Milestone 1: Data Simulation and Ingestion 
Objectives: 
• Simulate IoT data and push it into a file or message queue 
Tasks: 
1. Create a Python script to generate sensor data (every 5 seconds) 
2. Write to a file or Kafka/Stream (optional) 
Deliverables: 
• Python generator script 
• Sample data logs 
Milestone 2: Batch Data Pipeline (ETL) 
Objectives: 
• Ingest data, process it, and store it in a data warehouse 
Tasks: 
1. Use Python or Azure Data Factory to: 
o Extract data (CSV or stream) 
o Transform it (e.g., flag anomalies, average) 
o Load into SQL or Data Lake 
Deliverables: 
• ETL script or ADF pipeline 
• Processed dataset in storage 
Milestone 3: Streaming Pipeline with Alerts 
pg. 5 
AI & Data Science Track – Round 2  
Objectives: 
• Implement streaming analytics and alerting 
Tasks: 
1. Use Azure Stream Analytics or Apache Kafka to: 
o Process real-time data 
o Raise alerts for threshold breaches 
Deliverables: 
• Streaming pipeline setup 
• Alert logic code and output 
Milestone 4: Dashboard & Final Report 
Objectives: 
• Visualize metrics and summarize results 
Tasks: 
1. Create a real-time dashboard (Power BI, Streamlit, Grafana) 
2. Report on key findings and system performance 
Deliverables: 
• Dashboard screenshot/live demo 
• Final project report 
Final Milestone Summary: 
Milestone 
1. Data Simulation 
2. Batch ETL 
Key Deliverables 
Python generator 
ETL pipeline 
3. Streaming Analytics Real-time alerts 
4. Dashboard & Report Dashboard + PDF report 
